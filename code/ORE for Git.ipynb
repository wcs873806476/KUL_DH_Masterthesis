{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f0860-59a4-482f-a9a4-7d397a9ec1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set HTML folder path\n",
    "folder_path = './pages2/'\n",
    "file_list = sorted(glob.glob(os.path.join(folder_path, '*.htm')))\n",
    "\n",
    "all_data = []\n",
    "subject_count = {}  # Count the number of articles extracted from each discipline\n",
    "\n",
    "for filepath in file_list:\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    # Extract discipline name from file name (remove last digit)\n",
    "    subject_match = re.match(r'^(.+?)(\\d+)?\\.htm$', filename)\n",
    "    subject = subject_match.group(1).strip() if subject_match else 'Unknown'\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        html = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.find_all('article', class_='MuiCard-root')\n",
    "    \n",
    "    # Record the number of articles extracted\n",
    "    subject_count[subject] = subject_count.get(subject, 0) + len(articles)\n",
    "    print(f\"📄 {filename}（subject: {subject}）：extracted {len(articles)} articles\")\n",
    "\n",
    "    for article in articles:\n",
    "        research_type = 'N/A'\n",
    "        card_content = article.find('div', class_='MuiCardContent-root')\n",
    "        if card_content:\n",
    "            outer_divs = card_content.find_all('div', recursive=False)\n",
    "            if outer_divs:\n",
    "                inner_divs = outer_divs[0].find_all('div')\n",
    "                if inner_divs:\n",
    "                    research_type = inner_divs[0].text.strip()\n",
    "\n",
    "        title_tag = article.find('h3')\n",
    "        title = 'N/A'\n",
    "        if title_tag:\n",
    "            spans = title_tag.find_all('span')\n",
    "            if len(spans) == 1:\n",
    "                title = spans[0].text.strip()\n",
    "            elif len(spans) >= 2:\n",
    "                title = spans[-1].text.strip()\n",
    "            else:\n",
    "                title = title_tag.text.strip()\n",
    "\n",
    "        link = 'N/A'\n",
    "        parent = article\n",
    "        while parent:\n",
    "            if parent.name == 'a' and parent.has_attr('href'):\n",
    "                link = parent['href']\n",
    "                break\n",
    "            parent = parent.parent\n",
    "\n",
    "\n",
    "        review_info_tag = article.find('h4')\n",
    "        peer_review_status = review_info_tag.text.strip() if review_info_tag else 'N/A'\n",
    "\n",
    "        authors = []\n",
    "        author_section = article.find('span', string=lambda s: s and ('Author' in s or 'Authors' in s))\n",
    "        if author_section:\n",
    "            parent = author_section.find_parent()\n",
    "            author_spans = parent.find_all('span')[1:]\n",
    "            authors = [a.text.strip() for a in author_spans]\n",
    "\n",
    "        funders = []\n",
    "        funder_section = article.find('span', string=lambda s: s and ('Funder' in s or 'Funders' in s))\n",
    "        if funder_section:\n",
    "            parent = funder_section.find_parent()\n",
    "            funder_spans = parent.find_all('span')[1:]\n",
    "            funders = [f.text.strip() for f in funder_spans]\n",
    "\n",
    "        reviewers = []\n",
    "        reviewer_section = article.find('span', string=lambda s: s and ('Peer Reviewer' in s or 'Peer Reviewers' in s))\n",
    "        if reviewer_section:\n",
    "            parent = reviewer_section.find_parent()\n",
    "            reviewer_spans = parent.find_all('span')[1:]\n",
    "            if reviewer_spans:\n",
    "                reviewers = [r.text.strip() for r in reviewer_spans]\n",
    "            else:\n",
    "                sibling = reviewer_section.find_next_sibling()\n",
    "                if sibling and isinstance(sibling, str):\n",
    "                    reviewers = [sibling.strip()]\n",
    "\n",
    "        time_tag = article.find('time')\n",
    "        publish_date = time_tag['datetime'] if time_tag and time_tag.has_attr('datetime') else 'N/A'\n",
    "\n",
    "        all_data.append({\n",
    "            'filepath': os.path.basename(filepath),\n",
    "            'subject': subject,\n",
    "            'research_type': research_type,\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'peer_review_status': peer_review_status,\n",
    "            'authors': '; '.join(authors),\n",
    "            'funders': '; '.join(funders),\n",
    "            'reviewers': '; '.join(reviewers),\n",
    "            'publish_date': publish_date\n",
    "        })\n",
    "\n",
    "# 显示提取统计结果\n",
    "print(\"\\n Statistics on the number of articles extracted from each discipline：\")\n",
    "for subj, count in subject_count.items():\n",
    "    print(f\"{subj}: {count}\")\n",
    "\n",
    "# 保存为 CSV\n",
    "df = pd.DataFrame(all_data)\n",
    "df[\"publish_date\"] = pd.to_datetime(df[\"publish_date\"], errors=\"coerce\")\n",
    "df = df[df[\"publish_date\"] <= pd.Timestamp(\"2025-03-31\")]\n",
    "df.to_csv('1output.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\n results are recorded in 1output.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadfa5a7-44dc-4be1-90cd-41826bdd19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# 1. Read article titles and links from CSV files\n",
    "df = pd.read_csv(\"1output.csv\")\n",
    "article_info = list(zip(df['title'], df['article_link']))\n",
    "\n",
    "results = []     \n",
    "failures = []    \n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 2. grab information\n",
    "for idx, (title, link) in enumerate(article_info):\n",
    "    print(f\"\\n[{idx+1}] article：{title}\")\n",
    "    if link == \"N/A\":\n",
    "        print(\" skip:no link\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        time.sleep(5)  \n",
    "\n",
    "        if idx == 0:\n",
    "            input(\" click: Accept cookies，and enter...\")\n",
    "\n",
    "        # click Authors Tab（if have）\n",
    "        try:\n",
    "            author_tab = driver.find_element(By.XPATH, '//button[.//span[text()=\"Authors\"]]')\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", author_tab)\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"arguments[0].click();\", author_tab)\n",
    "            time.sleep(4)\n",
    "            print(\" success click Authors Tab\")\n",
    "        except Exception as e:\n",
    "            print(\" we dont find Authors\")\n",
    "\n",
    "        # wait authors information\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'div.MuiCard-root'))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(\" skip\")\n",
    "            failures.append({\"title\": title, \"link\": link, \"reasons\": \"Time out Exception\"})\n",
    "            continue\n",
    "\n",
    "        # grab authors information\n",
    "        try:\n",
    "            author_cards = driver.find_elements(By.CSS_SELECTOR, 'div.MuiCard-root')\n",
    "            if not author_cards:\n",
    "                print(\" do not find authors information\")\n",
    "                failures.append({\"title\": title, \"link\": link, \"reasons\": \"do not find authors information\"})\n",
    "                continue\n",
    "\n",
    "            for card in author_cards:\n",
    "                try:\n",
    "                    name_el = card.find_element(By.TAG_NAME, 'h3')\n",
    "                    author_name = name_el.text.strip()\n",
    "                except:\n",
    "                    author_name = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    children = card.find_elements(By.XPATH, \"./*\")\n",
    "                    affiliations = []\n",
    "                    for child in children:\n",
    "                        tag = child.tag_name.lower()\n",
    "                        text = child.text.strip()\n",
    "\n",
    "                        if tag == 'h5' and 'role' in text.lower():\n",
    "                            break\n",
    "\n",
    "                        if tag == 'p':\n",
    "                            if any(role in text.lower() for role in [\n",
    "                                'writing', 'formal analysis', 'data curation', 'investigation',\n",
    "                                'methodology', 'validation', 'resources', 'supervision', 'review', 'roles'\n",
    "                            ]):\n",
    "                                continue\n",
    "                            if ',' in text:\n",
    "                                affiliations.append(text)\n",
    "\n",
    "                    if affiliations:\n",
    "                        parts = affiliations[-1].rsplit(',', 1)\n",
    "                        institution = parts[0].strip()\n",
    "                        country = parts[1].strip()\n",
    "                    else:\n",
    "                        institution = \"N/A\"\n",
    "                        country = \"N/A\"\n",
    "\n",
    "                except Exception as e:\n",
    "                    institution = \"N/A\"\n",
    "                    country = \"N/A\"\n",
    "\n",
    "                print(f\" author_name: {author_name}\")\n",
    "                print(f\" institution: {institution}\")\n",
    "                print(f\" country: {country}\")\n",
    "                print(\"----\")\n",
    "\n",
    "                results.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"author_name\": author_name,\n",
    "                    \"institution\": institution,\n",
    "                    \"country\": country\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\" Failed to capture author card：\", e)\n",
    "            failures.append({\"title\": title, \"link\": link, \"reasons\": \"Failed to capture author card\"})\n",
    "\n",
    "\n",
    "    except Exception as outer_e:\n",
    "        print(f\" Fail to capture author card：{outer_e}\")\n",
    "        failures.append({\"title\": title, \"link\": link, \"reasons\": str(outer_e)})\n",
    "        continue\n",
    "\n",
    "# 3. store results\n",
    "driver.quit()\n",
    "\n",
    "# keep success data\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"2authors.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\n Author information has been saved to 2authors.csv\")\n",
    "\n",
    "# store fail results\n",
    "if failures:\n",
    "    df_failures = pd.DataFrame(failures)\n",
    "    df_failures.to_csv(\"failed_articles.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\" The article that failed to capture has been saved to failed_articles.csv\")\n",
    "else:\n",
    "    print(\" All articles have been successfully crawled with no record of failures!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66276c-cfa6-4763-9c99-a2f095dad0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Read the original CSV (note: comma-separated)\n",
    "df = pd.read_csv(\"1output.csv\", sep=\",\")\n",
    "\n",
    "# Convert the date column\n",
    "df[\"Publication Date\"] = pd.to_datetime(df[\"Publication Date\"], errors=\"coerce\")\n",
    "\n",
    "# Filter: Keep only articles published on or before 2025-03-31\n",
    "cutoff_date = pd.to_datetime(\"2025-03-31\")\n",
    "df = df[df[\"Publication Date\"] <= cutoff_date]\n",
    "\n",
    "# Prepare output results\n",
    "results = []\n",
    "\n",
    "# Regular expressions\n",
    "version_pattern = re.compile(r'version (\\d+)')\n",
    "review_pattern = re.compile(\n",
    "    r'(\\d+) (approved with reservations|not approved|approved)', \n",
    "    re.IGNORECASE\n",
    ")\n",
    "awaiting_pattern = re.compile(r'peer review: awaiting peer review', re.IGNORECASE)\n",
    "\n",
    "# Iterate over each row\n",
    "for idx, row in df.iterrows():\n",
    "    title = row.get(\"Title\", \"\")\n",
    "    link = row.get(\"Link\", \"\")\n",
    "    status = row.get(\"Peer Review Status\", \"\")\n",
    "\n",
    "    # 1. Maximum review version\n",
    "    versions = version_pattern.findall(status)\n",
    "    max_version = max(map(int, versions)) if versions else 0\n",
    "\n",
    "    # 2. Count each review status\n",
    "    approved = 0\n",
    "    reservations = 0\n",
    "    not_approved = 0\n",
    "    total_reviewers = 0\n",
    "\n",
    "    for count, label in review_pattern.findall(status):\n",
    "        count = int(count)\n",
    "        total_reviewers += count\n",
    "        label = label.lower()\n",
    "        if label == \"approved\":\n",
    "            approved += count\n",
    "        elif label == \"approved with reservations\":\n",
    "            reservations += count\n",
    "        elif label == \"not approved\":\n",
    "            not_approved += count\n",
    "\n",
    "    # 3. Whether there are still \"awaiting review\" records\n",
    "    has_awaiting = \"Yes\" if awaiting_pattern.search(status) else \"No\"\n",
    "\n",
    "    results.append([\n",
    "        title,\n",
    "        link,\n",
    "        max_version,\n",
    "        total_reviewers,\n",
    "        approved,\n",
    "        reservations,\n",
    "        not_approved,\n",
    "        has_awaiting\n",
    "    ])\n",
    "\n",
    "# Overwrite the original output file\n",
    "with open(\"3peer_review_summary_detailed.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"title\", \"link\", \"max_version\", \"total_reviewers\", \"approved\", \"reservations\", \"not_approved\", \"has_awaiting\"])\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(\" Done! Only records before or on 2025-03-31 were kept, and results saved to the output file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43dd42c-2dfc-4c21-afe5-c05555e975b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Chunk configuration\n",
    "chunk_size = 50\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv(\"1output.csv\", sep=\";\")\n",
    "titles = df[\"title\"].tolist()\n",
    "urls = df[\"link\"].tolist()\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "\n",
    "def extract_reviewer_info(title, url):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    # options.add_argument(\"--headless\")  # Run headless if needed\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the “Open Peer Review” section to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//h3[contains(text(), 'Open Peer Review')]\"))\n",
    "        )\n",
    "\n",
    "        time.sleep(random.uniform(1.2, 2.4))\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Check if status is \"awaiting peer review\"\n",
    "        awaiting_p = soup.find(\"p\", string=lambda x: x and \"awaiting peer review\" in x.lower())\n",
    "        if awaiting_p:\n",
    "            results.append({\n",
    "                \"title\": title,\n",
    "                \"link\": url,\n",
    "                \"name\": \"\",\n",
    "                \"affiliation\": \"\",\n",
    "                \"country\": \"\",\n",
    "                \"status\": \"Awaiting Review\"\n",
    "            })\n",
    "            return True\n",
    "\n",
    "        # Extract reviewer info normally\n",
    "        li_tags = soup.find_all(\"li\", class_=lambda c: c and \"MuiListItem-root\" in c and \"MuiListItem-gutters\" in c)\n",
    "        success_flag = False\n",
    "\n",
    "        for li in li_tags:\n",
    "            spans = li.find_all(\"span\", class_=lambda c: c and \"MuiTypography-body2\" in c)\n",
    "            if len(spans) < 2:\n",
    "                continue\n",
    "\n",
    "            name = spans[0].get_text(strip=True)\n",
    "            aff_text = spans[1].get_text(strip=True)\n",
    "            if aff_text.startswith(\",\"):\n",
    "                aff_text = aff_text[1:].strip()\n",
    "\n",
    "            parts = [p.strip() for p in aff_text.split(\",\")]\n",
    "            if len(parts) >= 2:\n",
    "                country = parts[-1]\n",
    "                institution = parts[0]\n",
    "            else:\n",
    "                country = \"\"\n",
    "                institution = aff_text\n",
    "\n",
    "            results.append({\n",
    "                \"title\": title,\n",
    "                \"link\": url,\n",
    "                \"name\": name,\n",
    "                \"affiliation\": institution,\n",
    "                \"country\": country,\n",
    "                \"status\": \"Normal\"\n",
    "            })\n",
    "            success_flag = True\n",
    "\n",
    "        return success_flag\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error: {title} → {url} → {e}\")\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Main loop in chunks\n",
    "total = len(titles)\n",
    "for start in range(0, total, chunk_size):\n",
    "    end = min(start + chunk_size, total)\n",
    "    print(f\"\\n Processing articles {start + 1} to {end}...\\n\")\n",
    "\n",
    "    for idx in range(start, end):\n",
    "        title = titles[idx]\n",
    "        url = urls[idx]\n",
    "\n",
    "        print(f\" Article {idx + 1}: {title}\")\n",
    "        time.sleep(random.uniform(1.1, 2.2))\n",
    "\n",
    "        success = extract_reviewer_info(title, url)\n",
    "\n",
    "        if not success:\n",
    "            retry_url = url.rstrip(\"/\") + \"/v1\"\n",
    "            print(f\" Trying fallback link /v1: {retry_url}\")\n",
    "            time.sleep(random.uniform(1.1, 2.2))\n",
    "\n",
    "            retry_success = extract_reviewer_info(title, retry_url)\n",
    "\n",
    "            if not retry_success:\n",
    "                print(f\" Final failure: {title}\")\n",
    "                errors.append({\n",
    "                    \"title\": title,\n",
    "                    \"original link\": url,\n",
    "                    \"tried link\": retry_url,\n",
    "                    \"error\": \"both attempts failed\"\n",
    "                })\n",
    "\n",
    "# Save successful data\n",
    "pd.DataFrame(results).to_csv(\"4reviewer_info_full.csv\", index=False)\n",
    "print(\"\\n Success data saved to 4reviewer_info_full.csv\")\n",
    "\n",
    "# Save error log\n",
    "if errors:\n",
    "    pd.DataFrame(errors).to_csv(\"4reviewer_info_errors.csv\", index=False)\n",
    "    print(\" Error log saved to 4reviewer_info_errors.csv\")\n",
    "else:\n",
    "    print(\" No failed records!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717eec6-dfd1-43e0-a85e-b533b673d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Set up browser options\n",
    "options = Options()\n",
    "# options.add_argument('--headless')  # Uncomment for headless mode\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--window-size=1920,1080')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Read input file\n",
    "df = pd.read_csv(\"1output.csv\")\n",
    "titles = df[\"title\"].tolist()\n",
    "urls = df[\"link\"].tolist()\n",
    "\n",
    "# Output CSV file\n",
    "with open(\"5reviewers_rounds.csv\", \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"title\", \"link\", \"version\", \"version date\", \"reviewer ID\", \"status\"])\n",
    "\n",
    "    for i, (title, url) in enumerate(zip(titles, urls)):\n",
    "        print(f\" Processing article {i+1}: {title}\")\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(2)  # Wait for page to load\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            version_headers = soup.find_all(\"th\", {\"class\": lambda x: x and \"MuiTableCell-head\" in x})\n",
    "\n",
    "            for th in version_headers:\n",
    "                # Check if this is a Version row\n",
    "                span = th.find(\"span\", {\"class\": lambda x: x and \"MuiButton-label\" in x})\n",
    "                if not span or \"version\" not in span.text:\n",
    "                    continue\n",
    "                version_text = span.text.strip()\n",
    "\n",
    "                # Find date\n",
    "                time_tag = th.find(\"time\")\n",
    "                version_date = time_tag[\"datetime\"].strip() if time_tag else \"\"\n",
    "\n",
    "                # Get all <td> cells in this version row\n",
    "                row = th.find_parent(\"tr\")\n",
    "                tds = row.find_all(\"td\")\n",
    "\n",
    "                # Check if all reviewer cells are empty (possibly \"awaiting\" status)\n",
    "                all_empty = True\n",
    "                for td in tds:\n",
    "                    td_html = str(td)\n",
    "                    if \"<title>\" in td_html:\n",
    "                        all_empty = False\n",
    "                        break\n",
    "\n",
    "                # If all reviewer cells are empty, try detecting \"awaiting peer review\" message\n",
    "                if all_empty:\n",
    "                    p = soup.find(\"p\", string=\"awaiting peer review\")\n",
    "                    if p:\n",
    "                        writer.writerow([title, url, version_text, version_date, \"\", \"awaiting\"])\n",
    "                        continue  # Skip to next version\n",
    "                    else:\n",
    "                        # Fallback: write empty reviewer rows\n",
    "                        for idx, td in enumerate(tds):\n",
    "                            reviewer_id = idx + 1\n",
    "                            writer.writerow([title, url, version_text, version_date, reviewer_id, \"\"])\n",
    "                        continue\n",
    "\n",
    "                # Otherwise, parse actual review statuses\n",
    "                for idx, td in enumerate(tds):\n",
    "                    reviewer_id = idx + 1\n",
    "                    td_html = str(td)\n",
    "\n",
    "                    # Strictly match empty reviewer structure\n",
    "                    if td_html.strip() == '<td class=\"MuiTableCell-root MuiTableCell-body\"></td>':\n",
    "                        next_td_html = (\n",
    "                            str(tds[idx + 1]).strip()\n",
    "                            if idx + 1 < len(tds)\n",
    "                            else \"\"\n",
    "                        )\n",
    "                        if next_td_html == '<td class=\"MuiTableCell-root MuiTableCell-body\"></td>':\n",
    "                            writer.writerow([title, url, version_text, version_date, reviewer_id, \"\"])\n",
    "                            continue\n",
    "\n",
    "                    # Match review status string\n",
    "                    if \"<title>Approved</title>\" in td_html:\n",
    "                        status = \"Approved\"\n",
    "                    elif \"<title>Approved with Reservations</title>\" in td_html:\n",
    "                        status = \"Approved with Reservations\"\n",
    "                    elif \"<title>Not Approved</title>\" in td_html:\n",
    "                        status = \"Not Approved\"\n",
    "                    else:\n",
    "                        status = \"\"\n",
    "\n",
    "                    writer.writerow([title, url, version_text, version_date, reviewer_id, status])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error ({title}): {e}\")\n",
    "            writer.writerow([title, url, \"ERROR\", str(e), \"\", \"\"])\n",
    "\n",
    "driver.quit()\n",
    "print(\" Output saved to: 5reviewers_rounds.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3585823a-f5d5-4a1d-bd7e-cdf2f98e6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Read full link dataset\n",
    "df = pd.read_csv(\"1output.csv\")  # Full file\n",
    "df_subset = df  # No slicing, process all\n",
    "\n",
    "# List of failed records\n",
    "failed_rows = []\n",
    "\n",
    "# Output file\n",
    "with open(\"6stats_all.csv\", mode=\"w\", newline='', encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"title\", \"url\", \"views\", \"downloads\", \"citations\"])\n",
    "\n",
    "    for idx, row in df_subset.iterrows():\n",
    "        title = row[\"title\"]\n",
    "        url = row[\"link\"]\n",
    "        print(f\" Processing article {idx+1}: {title}\")\n",
    "\n",
    "        # Create a new Chrome driver for each article\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        # chrome_options.add_argument(\"--headless\")  # Uncomment for headless mode\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"h3\"))\n",
    "            )\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            views = downloads = citations = \"0\"\n",
    "\n",
    "            for li in soup.find_all(\"li\"):\n",
    "                title_attr = li.get(\"title\", \"\")\n",
    "                if \"Page views\" in title_attr:\n",
    "                    views = li.get_text(strip=True).replace(\"views\", \"\")\n",
    "                elif \"downloads\" in title_attr:\n",
    "                    downloads = li.get_text(strip=True).replace(\"downloads\", \"\")\n",
    "                elif \"Crossref\" in title_attr:\n",
    "                    citations = li.get_text(strip=True).replace(\"citations\", \"\")\n",
    "\n",
    "            writer.writerow([title, url, views, downloads, citations])\n",
    "            print(f\" Success: {views} views, {downloads} downloads, {citations} citations\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error ({title}): {e}\")\n",
    "            writer.writerow([title, url, \"ERROR\", \"ERROR\", \"ERROR\"])\n",
    "            failed_rows.append([title, url, str(e)])  # Log failure\n",
    "\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "# Save failure log\n",
    "if failed_rows:\n",
    "    with open(\"6failed_stats.csv\", mode=\"w\", newline='', encoding=\"utf-8\") as f_error:\n",
    "        error_writer = csv.writer(f_error)\n",
    "        error_writer.writerow([\"title\", \"url\", \"error\"])\n",
    "        error_writer.writerows(failed_rows)\n",
    "\n",
    "print(\"All processing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62559b-3cc5-477e-a3df-8d758716479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the original file\n",
    "df = pd.read_csv(\"1output.csv\", sep=\";\")\n",
    "\n",
    "# Initialize two output lists\n",
    "funders_rows = []\n",
    "count_rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    title = row[\"title\"]\n",
    "    link = row[\"link\"]\n",
    "    pub_time = row[\"publish_date\"]\n",
    "    funders_raw = str(row[\"funders\"]).strip()\n",
    "\n",
    "    # Split and clean funders\n",
    "    if funders_raw.lower() in [\"nan\", \"\", \"none\"]:\n",
    "        funders_list = []\n",
    "    else:\n",
    "        funders_list = [f.strip() for f in funders_raw.split(\";\") if f.strip()]\n",
    "\n",
    "    count = len(funders_list)\n",
    "\n",
    "    # Record funder count per article\n",
    "    count_rows.append([title, link, pub_time, count])\n",
    "\n",
    "    # Record each funder entry\n",
    "    if count == 0:\n",
    "        funders_rows.append([title, link, pub_time, count, \"\"])\n",
    "    else:\n",
    "        for funder in funders_list:\n",
    "            funders_rows.append([title, link, pub_time, count, funder])\n",
    "\n",
    "# Save funder details (one row per funder)\n",
    "df_funders = pd.DataFrame(funders_rows, columns=[\"title\", \"link\", \"publish_date\", \"funder_count\", \"funder_name\"])\n",
    "df_funders.to_csv(\"7funders_extracted.csv\", index=False)\n",
    "\n",
    "# Save count of funders per article\n",
    "df_count = pd.DataFrame(count_rows, columns=[\"title\", \"link\", \"publish_date\", \"funder_count\"])\n",
    "df_count.to_csv(\"7funders_count_per_article.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
